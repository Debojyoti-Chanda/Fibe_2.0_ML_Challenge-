{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train.csv', encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv('test.csv', encoding='ISO-8859-1')"
      ],
      "metadata": {
        "id": "V3VCW5NmRzSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.head())\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YaLz3-rR1yb",
        "outputId": "7a7c4f6f-8f94-4340-8f50-75518659a4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text              target  \\\n",
            "0  python courses python courses, python exercise...  academic interests   \n",
            "1  the learning point open digital education. a r...  academic interests   \n",
            "2  tech news, latest technology, mobiles, laptops...  academic interests   \n",
            "3  the best it certification materials in usa | k...  academic interests   \n",
            "4  bioland scientific, for your research needs bi...  academic interests   \n",
            "\n",
            "   Word Count  \n",
            "0         125  \n",
            "1         147  \n",
            "2         143  \n",
            "3         364  \n",
            "4         176  \n",
            "                                                text  Word Count      Index\n",
            "0  equl offers enzyme assay kits, reagent mixture...         353  Article_0\n",
            "1  gauthmath: instant math questions solver for f...         112  Article_1\n",
            "2  Whats the No. 1 cause of blindness in older ad...         340  Article_2\n",
            "3  Surfers will ride a wave in the Amazon this we...         465  Article_3\n",
            "4  Why is the top of a leaf the most colorful, so...         269  Article_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_set = set(train_df['target'].unique())\n",
        "print(target_set)\n",
        "print(len(target_set))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrwcvBPMVzBJ",
        "outputId": "8ff3609d-430f-45df-b07f-1b30737b00c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'academic interests', 'pets', 'shopping', 'careers', 'travel', 'health', 'style and fashion', 'real estate', 'music and audio', 'television', 'family and relationships', 'sports', 'hobbies and interests', 'healthy living', 'pharmaceuticals, conditions, and symptoms', 'automotives', 'news and politics', 'arts and culture', 'food and drinks', 'home and garden', 'personal finance', 'movies', 'video gaming', 'technology and computing', 'books and literature', 'business and finance'}\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the default English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove essential stop words\n",
        "essential_words = {\"\"}\n",
        "stop_words = stop_words - essential_words"
      ],
      "metadata": {
        "id": "3pJ4rVuVvASW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "# 2. Text Cleaning\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove excess whitespace\n",
        "    word_tokens = word_tokenize(text)  # Tokenize the text\n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "    # return text\n",
        "\n",
        "# Apply the text cleaning function to the 'text' column\n",
        "train_data['text'] = train_data['text'].apply(clean_text)\n",
        "test_data['text'] = test_data['text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "pTe8nglMvE-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save cleaned data to CSV (to offload memory)\n",
        "train_data[['text', 'target']].to_csv('train_cleaned.csv', index=False)\n",
        "test_data[['text','Index']].to_csv('test_cleaned.csv', index=False)"
      ],
      "metadata": {
        "id": "N9NfkllqvIPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start\n"
      ],
      "metadata": {
        "id": "_V8xV55CTjnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/dataset/train_cleaned.csv\")\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/dataset/test_cleaned.csv\")"
      ],
      "metadata": {
        "id": "X7vr3ofpSxBW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Train Data Info:\")\n",
        "train_data.info()\n",
        "print(\"\\nTest Data Info:\")\n",
        "test_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUdSp2b3S7x2",
        "outputId": "7063a458-1de0-420b-f91a-1600c3cd5626"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 697527 entries, 0 to 697526\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   text    697527 non-null  object\n",
            " 1   target  697527 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 10.6+ MB\n",
            "\n",
            "Test Data Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 174382 entries, 0 to 174381\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   text    174382 non-null  object\n",
            " 1   Index   174382 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 2.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Vectorization using TF-IDF with more features and bigrams\n",
        "tfidf = TfidfVectorizer(max_features=9000, ngram_range=(1,1))\n",
        "\n",
        "X_train = tfidf.fit_transform(train_data['text'])\n",
        "X_test = tfidf.transform(test_data['text'])\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, train_data['target'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "nr8oc2BcYszC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Model Training - MLPClassifier (Neural Network)\n",
        "model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=20, verbose=True, batch_size=128, early_stopping=True)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions on Validation Set\n",
        "y_pred_val = model.predict(X_val)\n",
        "\n",
        "# 5. Final Predictions on Test Set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Prepare Submission File\n",
        "submission = pd.DataFrame({'Index': test_data['Index'], 'target': y_test_pred})\n",
        "submission.to_csv('submission.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9LiinVEYwSW",
        "outputId": "7ebeb7ca-6362-4531-a4eb-4ac2d9bffd55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.00776269\n",
            "Validation score: 0.768292\n",
            "Iteration 2, loss = 0.68981434\n",
            "Validation score: 0.783327\n",
            "Iteration 3, loss = 0.56645134\n",
            "Validation score: 0.789993\n",
            "Iteration 4, loss = 0.46675756\n",
            "Validation score: 0.792807\n",
            "Iteration 5, loss = 0.37592521\n",
            "Validation score: 0.790334\n",
            "Iteration 6, loss = 0.29342417\n",
            "Validation score: 0.785513\n",
            "Iteration 7, loss = 0.21983977\n",
            "Validation score: 0.781105\n",
            "Iteration 8, loss = 0.15961435\n",
            "Validation score: 0.777449\n",
            "Iteration 9, loss = 0.11737469\n",
            "Validation score: 0.775084\n",
            "Iteration 10, loss = 0.09163250\n",
            "Validation score: 0.771141\n",
            "Iteration 11, loss = 0.07804189\n",
            "Validation score: 0.771733\n",
            "Iteration 12, loss = 0.07171462\n",
            "Validation score: 0.771374\n",
            "Iteration 13, loss = 0.06765309\n",
            "Validation score: 0.769224\n",
            "Iteration 14, loss = 0.06476510\n",
            "Validation score: 0.768471\n",
            "Iteration 15, loss = 0.06426690\n",
            "Validation score: 0.770460\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "# y_pred_val = model.predict(X_val)\n",
        "# Assuming you have 'actual' and 'predicted' variables defined with the true and predicted labels\n",
        "score = max(0, 100*metrics.f1_score(y_val, y_pred_val, average='weighted'))\n",
        "\n",
        "print(f\"Score: {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgA3UP6NYy_y",
        "outputId": "7d8cb11a-e5bb-4f36-d87c-d63865cee9a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 79.16920151163005\n"
          ]
        }
      ]
    }
  ]
}